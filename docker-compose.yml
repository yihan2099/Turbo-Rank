version: "3.9"

services:
  trt:
    build:
      context: .
      dockerfile: deploy/docker/trt.Dockerfile
    image: turbo-rank-trt:latest
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./models:/workspace/turbo-rank/models:ro
    ports:
      - "50051:50051"
    # keep container alive
    command: ["bash", "-c", "while true; do sleep 3600; done"]
    # interactive shell you can also add:
    stdin_open: true
    tty: true


  ort:
    build:
      context: .
      dockerfile: deploy/docker/ort.Dockerfile
    image: turbo-rank-ort:latest
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./models:/workspace/turbo-rank/models:ro
    ports:
      - "50052:50051"   # avoid port clash
    # keep container alive
    command: ["bash", "-c", "while true; do sleep 3600; done"]
    # interactive shell you can also add:
    stdin_open: true
    tty: true

  triton:
    build:
      context: .
      dockerfile: server/docker/triton.dockerfile
    image: turbo-rank-triton:latest
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      # If you prefer mounting at runtime instead of bake-in
      - ./server/models:/models:ro
    ports:
      - "8000:8000"
      - "8001:8001"
      - "8002:8002"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 30s
      timeout: 5s
      retries: 3