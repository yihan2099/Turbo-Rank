{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa98a785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.path:\n",
      "/hdd/yihan/projects/Turbo-Rank/turbo_rank\n",
      "/usr/lib/python310.zip\n",
      "/usr/lib/python3.10\n",
      "/usr/lib/python3.10/lib-dynload\n",
      "\n",
      "/hdd/yihan/projects/Turbo-Rank/.venv/lib/python3.10/site-packages\n",
      "/hdd/yihan/projects/Turbo-Rank\n",
      "PYTHONPATH:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# verify sys.path\n",
    "print(\"sys.path:\")\n",
    "print(\"\\n\".join(sys.path))\n",
    "\n",
    "# verify pythonpath\n",
    "print(\"PYTHONPATH:\")\n",
    "print(os.environ.get(\"PYTHONPATH\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe9e0035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow \n",
    "import sys \n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5010\")\n",
    "\n",
    "sys.path.insert(0, \"/hdd/yihan/projects/Turbo-Rank/turbo_rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d542270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 7/7 [00:00<00:00, 114.57it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Registered model artifacts downloaded to: /hdd/yihan/projects/Turbo-Rank/notebooks/NRMS_v1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from mlflow.artifacts import download_artifacts\n",
    "\n",
    "# URI of your registered model\n",
    "model_uri = \"models:/NRMS/1\"\n",
    "# Local directory to receive the files\n",
    "dst_path  = \"./NRMS_v1\"\n",
    "\n",
    "local_dir = download_artifacts(\n",
    "    artifact_uri=model_uri,\n",
    "    dst_path=dst_path\n",
    ")\n",
    "print(f\"✅ Registered model artifacts downloaded to: {local_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ec5d43",
   "metadata": {},
   "source": [
    "## torch.compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8395f6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded from models:/NRMS/3 to cuda:0\n",
      "✅ Model compiled\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): NRMSModel(\n",
       "    (news_encoder): NewsEncoder(\n",
       "      (embedding): Embedding(54021, 256, padding_idx=0)\n",
       "      (self_attn): _ScaledDotSelfAttention(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (additive): Linear(in_features=256, out_features=1, bias=False)\n",
       "    )\n",
       "    (user_encoder): UserEncoder(\n",
       "      (self_attn): _ScaledDotSelfAttention(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (additive): Linear(in_features=256, out_features=1, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5010\")\n",
    "\n",
    "# set sys.path\n",
    "sys.path.insert(0, \"/hdd/yihan/projects/Turbo-Rank/turbo_rank\")\n",
    "\n",
    "model_uri = \"models:/NRMS/3\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 1. Load\n",
    "model = mlflow.pytorch.load_model(model_uri, map_location=device) \n",
    "print(f\"✅ Model loaded from {model_uri} to {device}\")\n",
    "\n",
    "# 2. (Optional) compile model from top level \n",
    "if torch.cuda.is_available() and not isinstance(model, torch._dynamo.OptimizedModule):\n",
    "    model = torch.compile(model, mode=\"max-autotune\", fullgraph=True)\n",
    "    print(\"✅ Model compiled\")\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6559386d",
   "metadata": {},
   "source": [
    "### Sanity-check inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8045dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hdd/yihan/projects/Turbo-Rank/.venv/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "E0503 13:37:28.035000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] Runtime error during autotuning: \n",
      "E0503 13:37:28.035000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. \n",
      "E0503 13:37:28.035000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] Ignoring this choice.\n",
      "E0503 13:37:28.126000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] Runtime error during autotuning: \n",
      "E0503 13:37:28.126000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. \n",
      "E0503 13:37:28.126000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] Ignoring this choice.\n",
      "E0503 13:37:28.260000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] Runtime error during autotuning: \n",
      "E0503 13:37:28.260000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. \n",
      "E0503 13:37:28.260000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] Ignoring this choice.\n",
      "AUTOTUNE mm(600x256, 256x256)\n",
      "  triton_mm_43 0.0287 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4\n",
      "  triton_mm_45 0.0287 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4\n",
      "  triton_mm_48 0.0287 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8\n",
      "  triton_mm_51 0.0287 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4\n",
      "  triton_mm_44 0.0287 ms 99.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8\n",
      "  triton_mm_34 0.0297 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=1, num_warps=2\n",
      "  triton_mm_35 0.0297 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4\n",
      "  triton_mm_36 0.0297 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8\n",
      "  triton_mm_37 0.0297 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8\n",
      "  triton_mm_38 0.0297 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4\n",
      "SingleProcess AUTOTUNE benchmarking takes 0.4743 seconds and 0.0008 seconds precompiling for 20 choices\n",
      "E0503 13:37:29.854000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] Runtime error during autotuning: \n",
      "E0503 13:37:29.854000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. \n",
      "E0503 13:37:29.854000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] Ignoring this choice.\n",
      "E0503 13:37:29.944000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] Runtime error during autotuning: \n",
      "E0503 13:37:29.944000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. \n",
      "E0503 13:37:29.944000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] Ignoring this choice.\n",
      "E0503 13:37:30.081000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] Runtime error during autotuning: \n",
      "E0503 13:37:30.081000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. \n",
      "E0503 13:37:30.081000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] Ignoring this choice.\n",
      "AUTOTUNE mm(120x256, 256x256)\n",
      "  triton_mm_98 0.0276 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4\n",
      "  triton_mm_88 0.0287 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4\n",
      "  triton_mm_90 0.0287 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8\n",
      "  triton_mm_91 0.0287 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4\n",
      "  triton_mm_92 0.0287 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4\n",
      "  triton_mm_93 0.0287 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4\n",
      "  triton_mm_96 0.0287 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4\n",
      "  triton_mm_97 0.0296 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8\n",
      "  triton_mm_100 0.0297 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4\n",
      "  triton_mm_87 0.0297 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=1, num_warps=2\n",
      "SingleProcess AUTOTUNE benchmarking takes 0.4562 seconds and 0.0005 seconds precompiling for 20 choices\n",
      "E0503 13:37:31.337000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] Runtime error during autotuning: \n",
      "E0503 13:37:31.337000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 122880, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. \n",
      "E0503 13:37:31.337000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] Ignoring this choice.\n",
      "AUTOTUNE mm(20x256, 256x256)\n",
      "  triton_mm_70 0.0287 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=1, num_warps=2\n",
      "  triton_mm_71 0.0287 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4\n",
      "  triton_mm_72 0.0287 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8\n",
      "  triton_mm_73 0.0287 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4\n",
      "  triton_mm_74 0.0287 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4\n",
      "  triton_mm_75 0.0287 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4\n",
      "  triton_mm_80 0.0287 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8\n",
      "  triton_mm_86 0.0287 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8\n",
      "  triton_mm_76 0.0297 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4\n",
      "  triton_mm_77 0.0297 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8\n",
      "SingleProcess AUTOTUNE benchmarking takes 0.4214 seconds and 0.0005 seconds precompiling for 18 choices\n",
      "E0503 13:37:32.905000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] Runtime error during autotuning: \n",
      "E0503 13:37:32.905000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 110592, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. \n",
      "E0503 13:37:32.905000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] Ignoring this choice.\n",
      "AUTOTUNE bmm(30x4x256, 30x256x768)\n",
      "  triton_bmm_2 0.0287 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4\n",
      "  triton_bmm_3 0.0287 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=2\n",
      "  triton_bmm_5 0.0287 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4\n",
      "  triton_bmm_6 0.0287 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4\n",
      "  triton_bmm_7 0.0287 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4\n",
      "  triton_bmm_9 0.0287 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4\n",
      "  triton_bmm_10 0.0287 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8\n",
      "  triton_bmm_11 0.0287 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4\n",
      "  triton_bmm_14 0.0287 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4\n",
      "  triton_bmm_0 0.0287 ms 99.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=1, num_warps=2\n",
      "SingleProcess AUTOTUNE benchmarking takes 0.4287 seconds and 0.0027 seconds precompiling for 18 choices\n",
      "E0503 13:37:33.331000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] Runtime error during autotuning: \n",
      "E0503 13:37:33.331000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 122880, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. \n",
      "E0503 13:37:33.331000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] Ignoring this choice.\n",
      "AUTOTUNE bmm(30x20x256, 30x256x768)\n",
      "  triton_bmm_30 0.0287 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4\n",
      "  triton_bmm_31 0.0287 ms 99.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8\n",
      "  triton_bmm_26 0.0287 ms 99.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4\n",
      "  triton_bmm_17 0.0297 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=1, num_warps=2\n",
      "  triton_bmm_19 0.0297 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8\n",
      "  triton_bmm_22 0.0297 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4\n",
      "  triton_bmm_23 0.0297 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4\n",
      "  triton_bmm_24 0.0297 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8\n",
      "  triton_bmm_27 0.0297 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8\n",
      "  triton_bmm_28 0.0297 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4\n",
      "SingleProcess AUTOTUNE benchmarking takes 0.4136 seconds and 0.0004 seconds precompiling for 18 choices\n",
      "E0503 13:37:33.747000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] Runtime error during autotuning: \n",
      "E0503 13:37:33.747000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 110592, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. \n",
      "E0503 13:37:33.747000 655424 torch/_inductor/select_algorithm.py:2100] [0/0] Ignoring this choice.\n",
      "AUTOTUNE bmm(5x4x256, 5x256x768)\n",
      "  triton_bmm_66 0.0276 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4\n",
      "  triton_bmm_53 0.0287 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=1, num_warps=2\n",
      "  triton_bmm_54 0.0287 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=2\n",
      "  triton_bmm_55 0.0287 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4\n",
      "  triton_bmm_58 0.0287 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4\n",
      "  triton_bmm_59 0.0287 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4\n",
      "  triton_bmm_61 0.0287 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4\n",
      "  triton_bmm_62 0.0287 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4\n",
      "  triton_bmm_63 0.0287 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8\n",
      "  triton_bmm_67 0.0287 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4\n",
      "SingleProcess AUTOTUNE benchmarking takes 0.4153 seconds and 0.0004 seconds precompiling for 18 choices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ scores.shape: torch.Size([4])\n",
      "→ sample scores: tensor([-0.9996, -0.9948, -0.9995, -0.9984], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# -- configure dummy inputs --\n",
    "B, H, L = 4, 5, 30                          # batch, history length, token length\n",
    "vocab_size = 54021                         # must match your NewsEncoder\n",
    "device     = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cand = torch.randint(1, vocab_size, (B, L), device=device)\n",
    "hist = torch.randint(1, vocab_size, (B, H, L), device=device)\n",
    "\n",
    "# -- inference --\n",
    "model.to(device)\n",
    "with torch.inference_mode(), torch.amp.autocast(device_type='cuda'):\n",
    "    scores = model(cand, hist)\n",
    "\n",
    "print(\"→ scores.shape:\", scores.shape)     # should be (B,)\n",
    "print(\"→ sample scores:\", scores[:B])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd578b2",
   "metadata": {},
   "source": [
    "### Measure end-to-end latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "884e94d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "These live storage data ptrs are in the cudagraph pool but not accounted for as an output of cudagraph trees: \n\nData Pointer: 139803991480320, history: \n\nData Pointer: 139803991485440, history: \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode(), torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# warm-up\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m         _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# timed pass\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
      "File \u001b[0;32m/hdd/yihan/projects/Turbo-Rank/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/hdd/yihan/projects/Turbo-Rank/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/hdd/yihan/projects/Turbo-Rank/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:655\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[0;32m/hdd/yihan/projects/Turbo-Rank/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/hdd/yihan/projects/Turbo-Rank/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/hdd/yihan/projects/Turbo-Rank/turbo_rank/models/nrms.py:147\u001b[0m, in \u001b[0;36mNRMSModel.forward\u001b[0;34m(self, candidate_tokens, history_tokens)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_encoder \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_encoder)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    149\u001b[0m     candidate_tokens: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    150\u001b[0m     history_tokens: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    151\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    152\u001b[0m     B, H, L \u001b[38;5;241m=\u001b[39m history_tokens\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# Encode candidate news\u001b[39;00m\n",
      "File \u001b[0;32m/hdd/yihan/projects/Turbo-Rank/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    836\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    840\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/hdd/yihan/projects/Turbo-Rank/.venv/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1201\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.forward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m   1199\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(params_flat)\n\u001b[1;32m   1200\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(runtime_args)\n\u001b[0;32m-> 1201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/hdd/yihan/projects/Turbo-Rank/.venv/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:328\u001b[0m, in \u001b[0;36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m grad_enabled:\n\u001b[1;32m    327\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_set_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 328\u001b[0m     all_outs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompiled_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m grad_enabled:\n",
      "File \u001b[0;32m/hdd/yihan/projects/Turbo-Rank/.venv/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[0m, in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 126\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m         )\n",
      "File \u001b[0;32m/hdd/yihan/projects/Turbo-Rank/.venv/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:689\u001b[0m, in \u001b[0;36mEffectTokensWrapper.post_compile.<locals>.inner_fn\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    686\u001b[0m     args \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m([\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m num_tokens), \u001b[38;5;241m*\u001b[39margs]\n\u001b[1;32m    687\u001b[0m     old_args\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m--> 689\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;66;03m# Inductor cache DummyModule can return None\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/hdd/yihan/projects/Turbo-Rank/.venv/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:495\u001b[0m, in \u001b[0;36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001b[0;34m(runtime_args)\u001b[0m\n\u001b[1;32m    488\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_functionalized_rng_runtime_epilogue(\n\u001b[1;32m    489\u001b[0m         runtime_metadata,\n\u001b[1;32m    490\u001b[0m         out,\n\u001b[1;32m    491\u001b[0m         \u001b[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001b[39;00m\n\u001b[1;32m    492\u001b[0m         runtime_metadata\u001b[38;5;241m.\u001b[39mnum_forward_returns,\n\u001b[1;32m    493\u001b[0m     )\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m--> 495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruntime_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/hdd/yihan/projects/Turbo-Rank/.venv/lib/python3.10/site-packages/torch/_inductor/output_code.py:460\u001b[0m, in \u001b[0;36mCompiledFxGraph.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     get_runtime_metrics_context()\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m/hdd/yihan/projects/Turbo-Rank/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:1372\u001b[0m, in \u001b[0;36mcudagraphify.<locals>.run\u001b[0;34m(new_inputs)\u001b[0m\n\u001b[1;32m   1370\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_utils\u001b[38;5;241m.\u001b[39mpreserve_rng_state():\n\u001b[1;32m   1371\u001b[0m         compiled_fn \u001b[38;5;241m=\u001b[39m cudagraphify_fn(model, new_inputs, static_input_idxs)\n\u001b[0;32m-> 1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/hdd/yihan/projects/Turbo-Rank/.venv/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py:371\u001b[0m, in \u001b[0;36mcudagraphify_impl.<locals>.deferred_cudagraphify\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m    369\u001b[0m fn \u001b[38;5;241m=\u001b[39m fn_cache\u001b[38;5;241m.\u001b[39mget(int_key)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m int_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    374\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecording cudagraph tree for graph without symints\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/hdd/yihan/projects/Turbo-Rank/.venv/lib/python3.10/site-packages/torch/_inductor/utils.py:2404\u001b[0m, in \u001b[0;36malign_inputs_from_check_idxs.<locals>.run\u001b[0;34m(new_inputs)\u001b[0m\n\u001b[1;32m   2402\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(new_inputs: \u001b[38;5;28mlist\u001b[39m[InputType]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   2403\u001b[0m     copy_misaligned_inputs(new_inputs, inputs_to_check)\n\u001b[0;32m-> 2404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/hdd/yihan/projects/Turbo-Rank/.venv/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py:1997\u001b[0m, in \u001b[0;36mCUDAGraphTreeManager.run\u001b[0;34m(self, new_inputs, function_id)\u001b[0m\n\u001b[1;32m   1995\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid_to_mode[function_id]\n\u001b[1;32m   1996\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompile_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid_to_compile_id[function_id]\n\u001b[0;32m-> 1997\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1999\u001b[0m \u001b[38;5;66;03m# The forwards are only pending following invocation, not before\u001b[39;00m\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m CompilationMode\u001b[38;5;241m.\u001b[39mFORWARD:\n",
      "File \u001b[0;32m/hdd/yihan/projects/Turbo-Rank/.venv/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py:2175\u001b[0m, in \u001b[0;36mCUDAGraphTreeManager._run\u001b[0;34m(self, new_inputs, function_id)\u001b[0m\n\u001b[1;32m   2171\u001b[0m \u001b[38;5;66;03m# now, we are in a recording state !\u001b[39;00m\n\u001b[1;32m   2172\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed_cudagraph(\n\u001b[1;32m   2173\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDAGraphTreeManager.record_function\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompile_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[1;32m   2174\u001b[0m ):\n\u001b[0;32m-> 2175\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/hdd/yihan/projects/Turbo-Rank/.venv/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py:2211\u001b[0m, in \u001b[0;36mCUDAGraphTreeManager.record_function\u001b[0;34m(self, new_inputs, function_id)\u001b[0m\n\u001b[1;32m   2205\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecording function \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m of graph recording id \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2207\u001b[0m     function_id\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m   2208\u001b[0m     graph_id\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m   2209\u001b[0m )\n\u001b[1;32m   2210\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[0;32m-> 2211\u001b[0m node \u001b[38;5;241m=\u001b[39m \u001b[43mCUDAGraphNode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2212\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mids_to_funcs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfunction_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2214\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_node\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda_graphs_thread_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mids_to_stack_traces\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfunction_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2224\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroots[function_id]\u001b[38;5;241m.\u001b[39mappend(node)\n",
      "File \u001b[0;32m/hdd/yihan/projects/Turbo-Rank/.venv/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py:1021\u001b[0m, in \u001b[0;36mCUDAGraphNode.__init__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;66;03m# Cleared after recording\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed_cudagraph(\n\u001b[1;32m   1019\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDAGraphNode.record\u001b[39m\u001b[38;5;124m\"\u001b[39m, compile_id, mode, dynamo_compile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m ):\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecording_outputs: Optional[OutputType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrapped_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecording_inputs\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_metadata: OutputList[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;66;03m# As with inputs, we do not want to keep the outputs permanently alive because that would prevent\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;66;03m# their memory being reclaimed in subsequent cuda graph recordings. We record the tensor metadata\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;66;03m# needed to reconstruct instead.\u001b[39;00m\n",
      "File \u001b[0;32m/hdd/yihan/projects/Turbo-Rank/.venv/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py:1246\u001b[0m, in \u001b[0;36mCUDAGraphNode._record\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1236\u001b[0m     memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1237\u001b[0m         [] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mpath_live_weakrefs())\n\u001b[1;32m   1238\u001b[0m     )\n\u001b[1;32m   1239\u001b[0m     memory \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1240\u001b[0m         StorageWeakRefWrapper(elem)\n\u001b[1;32m   1241\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, elem \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(inputs)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m elem\u001b[38;5;241m.\u001b[39muntyped_storage()\u001b[38;5;241m.\u001b[39mdata_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1245\u001b[0m     ]\n\u001b[0;32m-> 1246\u001b[0m     \u001b[43mcheck_memory_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda_graphs_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[1;32m   1249\u001b[0m     preserve_rng_state(),\n\u001b[1;32m   1250\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1258\u001b[0m     get_history_recording(),\n\u001b[1;32m   1259\u001b[0m ):\n\u001b[1;32m   1260\u001b[0m     static_outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "File \u001b[0;32m/hdd/yihan/projects/Turbo-Rank/.venv/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py:1834\u001b[0m, in \u001b[0;36mcheck_memory_pool\u001b[0;34m(device, pool_id, live_storages_ptrs)\u001b[0m\n\u001b[1;32m   1829\u001b[0m formatted_s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(formatted)\n\u001b[1;32m   1830\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1831\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThese live storage data ptrs are in the cudagraph pool but not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccounted for as an output of cudagraph trees: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mformatted_s\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1833\u001b[0m )\n\u001b[0;32m-> 1834\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: These live storage data ptrs are in the cudagraph pool but not accounted for as an output of cudagraph trees: \n\nData Pointer: 139803991480320, history: \n\nData Pointer: 139803991485440, history: \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "with torch.inference_mode(), torch.amp.autocast(device_type='cuda'):\n",
    "    # warm-up\n",
    "    for _ in range(10):\n",
    "        _ = model(cand, hist)\n",
    "\n",
    "    # timed pass\n",
    "    t0 = time.perf_counter()\n",
    "    _  = model(cand, hist)\n",
    "    torch.cuda.synchronize()          # barrier for fair timing\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "print(f\"One forward pass: {(t1-t0)*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f73420",
   "metadata": {},
   "source": [
    "## ONNX and ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e644e323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded from models:/NRMS/3 to cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NRMSModel(\n",
       "  (news_encoder): NewsEncoder(\n",
       "    (embedding): Embedding(54021, 256, padding_idx=0)\n",
       "    (self_attn): _ScaledDotSelfAttention(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (additive): Linear(in_features=256, out_features=1, bias=False)\n",
       "  )\n",
       "  (user_encoder): UserEncoder(\n",
       "    (self_attn): _ScaledDotSelfAttention(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (additive): Linear(in_features=256, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5010\")\n",
    "\n",
    "# set sys.path\n",
    "sys.path.insert(0, \"/hdd/yihan/projects/Turbo-Rank/turbo_rank\")\n",
    "\n",
    "model_uri = \"models:/NRMS/3\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 1. Load\n",
    "model = mlflow.pytorch.load_model(model_uri, map_location=device) \n",
    "print(f\"✅ Model loaded from {model_uri} to {device}\")\n",
    "\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6630abeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0505 16:26:53.032000 1057438 torch/onnx/_internal/exporter/_registration.py:103] torchvision is not installed. Skipping torchvision::nms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `NRMSModel([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `NRMSModel([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hdd/yihan/projects/Turbo-Rank/.venv/lib/python3.10/site-packages/torch/onnx/_internal/exporter/_dynamic_shapes.py:265: UserWarning: # The axis name: batch will not be used, since it shares the same shape constraints with another axis: batch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied 59 of general pattern rewrite rules.\n",
      "ONNX expects symbolic dims: {'candidate_tokens': ['batch', 'seq_len'], 'history_tokens': ['batch', 'hist_len', 'seq_len']}\n",
      "Out1 shape: (2,)\n",
      "Out1: [-0.998402  -0.9990822]\n",
      "Out2 shape: (1,)\n",
      "Out2: [-0.9974028]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "# ── 1. Define & export your model (as you already do) ──────────────\n",
    "B, H, L      = 4, 5, 30\n",
    "vocab_size   = 54021\n",
    "device       = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "cand = torch.randint(1, vocab_size, (B, L), device=device)\n",
    "hist = torch.randint(1, vocab_size, (B, H, L), device=device)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (cand, hist),\n",
    "    \"../models/onnx/nrms.onnx\",\n",
    "    input_names=[\"candidate_tokens\", \"history_tokens\"],\n",
    "    output_names=[\"score\"],\n",
    "    opset_version=18,\n",
    "    dynamic_shapes={\n",
    "        \"candidate_tokens\": {0: \"batch\", 1: \"seq_len\"},            # (B, L)\n",
    "        \"history_tokens\": {0: \"batch\", 1: \"hist_len\", 2: \"seq_len\"}\n",
    "    },\n",
    "    dynamo=True,\n",
    ")\n",
    "\n",
    "# ── 2. Load the ONNX model ─────────────────────────────────────────\n",
    "sess = ort.InferenceSession(\"nrms.onnx\")\n",
    "inp_meta = {i.name: i.shape for i in sess.get_inputs()}\n",
    "print(\"ONNX expects symbolic dims:\", inp_meta)\n",
    "# e.g. {'cand': ['batch', 'token_len'], 'hist': ['batch', 'hist_len', 'token_len']}\n",
    "\n",
    "# ── 3. Inference example A: B=2, H=4, L=50 ────────────────────────\n",
    "B1, H1, L1 = 2, 4, 50\n",
    "cand1 = np.random.randint(1, vocab_size, (B1, L1), dtype=np.int64)\n",
    "hist1 = np.random.randint(1, vocab_size, (B1, H1, L1), dtype=np.int64)\n",
    "out1 = sess.run([\"score\"], {\"candidate_tokens\": cand1, \"history_tokens\": hist1})[0]\n",
    "print(\"Out1 shape:\", out1.shape)  # → (2,)\n",
    "print(\"Out1:\", out1)            \n",
    "\n",
    "# ── 4. Inference example B: B=1, H=10, L=30 ───────────────────────\n",
    "B2, H2, L2 = 1, 10, 30\n",
    "cand2 = np.random.randint(1, vocab_size, (B2, L2), dtype=np.int64)\n",
    "hist2 = np.random.randint(1, vocab_size, (B2, H2, L2), dtype=np.int64)\n",
    "out2 = sess.run(None, {\"candidate_tokens\": cand2, \"history_tokens\": hist2})[0]\n",
    "print(\"Out2 shape:\", out2.shape)  # → (1,)\n",
    "print(\"Out2:\", out2)           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0973a289",
   "metadata": {},
   "source": [
    "## ONNX Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbd7815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "quantize_dynamic(\n",
    "    \"../models/onnx/nrms.onnx\", \n",
    "    \"../models/onnx/quant/nrms.quant.onnx\",\n",
    "    weight_type=QuantType.QInt8\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
